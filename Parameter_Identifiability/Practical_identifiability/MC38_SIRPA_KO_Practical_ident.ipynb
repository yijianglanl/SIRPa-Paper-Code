{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMPrjzE5-Qw3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy.optimize import minimize, differential_evolution\n",
        "from scipy.stats import norm, chi2\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# -----------------------------\n",
        "# ODE model\n",
        "# -----------------------------\n",
        "def ode_rhs(t, y, x):\n",
        "    dydt1 = 0.39562*y[0]*(1-y[0]/890447238) - 1.06870e-07*y[0]*y[1] - x[0]*y[0]*y[2]\n",
        "    dydt2 = 75198 - 0.00122*y[1] -1.59893e-09*y[0]*y[1]\n",
        "    dydt3 = x[1] - x[2]*y[2] - x[3]*y[0]*y[2]\n",
        "    return [dydt1, dydt2, dydt3]\n",
        "\n",
        "def solve_dataset(t_obs, x, y10, y20, y30):\n",
        "    sol = solve_ivp(lambda t, y: ode_rhs(t, y, x),\n",
        "                    (t_obs[0], t_obs[-1]),\n",
        "                    [y10, y20, y30],\n",
        "                    t_eval=t_obs, method=\"BDF\", rtol=1e-6, atol=1e-8)\n",
        "    return sol.y[0]  # compare y1 to experimental data\n",
        "\n",
        "# -----------------------------\n",
        "# Experimental data\n",
        "# -----------------------------\n",
        "datasets = [\n",
        "    (np.array([18,20,22,24,26,28,30,32], float),\n",
        "     np.array([1.51e6,1.642e7,3.019e7,6.159e7,1.1406e8,1.8944e8,3.1565e8,4.7166e8], float),\n",
        "    1432029.87),\n",
        "    (np.array([10,12,14,16,18,20,22,24,26], float),\n",
        "     np.array([2.02e6,6.61e6,4.046e7,1.1102e8,2.4265e8,4.7158e8,7.7838e8,1.17088e9,1.26995e9], float),\n",
        "      14527932.65),\n",
        "    (np.array([6,8,10,12,14,16,18,20], float),\n",
        "     np.array([6.23e6,4.28e7,1.0936e8,2.1472e8,4.2609e8,7.2969e8,1.11062e9,1.376525e9], float),\n",
        "    13558129.72),\n",
        "    (np.array([2,4,6,8,10,12,14], float),\n",
        "     np.array([2.244e7,8.769e7,2.3401e8,4.4702e8,7.2802e8,1.13022e9,1.50725e9], float),\n",
        "     36022479.73)\n",
        "]\n",
        "y20 = 94088.81\n",
        "y30 = 51639.71\n",
        "\n",
        "theta_true = np.array([1.0000003423976e-07, 1.09944329169823e-07,  0.63506536118549,9.99991261886903e-07])\n",
        "\n",
        "# -----------------------------\n",
        "# Generate noisy data\n",
        "# -----------------------------\n",
        "CV, SigmaFloor = 0.05, 1e5\n",
        "exp_data, sigma_data = [], []\n",
        "for t_obs, vals, y10 in datasets:\n",
        "    clean = solve_dataset(t_obs, theta_true, y10, y20, y30)\n",
        "    sigma_vals = np.maximum(CV*clean, SigmaFloor)\n",
        "    noisy = clean + np.random.normal(0, sigma_vals)\n",
        "    exp_data.append(noisy)\n",
        "    sigma_data.append(sigma_vals)\n",
        "\n",
        "print(\"Noisy experimental data generated.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Log-likelihood\n",
        "# -----------------------------\n",
        "def loglikelihood(theta):\n",
        "    ll = 0.0\n",
        "    for (t_obs, data, y10), noisy, sigmas in zip(datasets, exp_data, sigma_data):\n",
        "        pred = solve_dataset(t_obs, theta, y10, y20, y30)\n",
        "        resid = noisy - pred\n",
        "        ll += np.sum(norm.logpdf(resid, 0, sigmas))\n",
        "    return ll\n",
        "\n",
        "def neg_ll(theta):\n",
        "    return -loglikelihood(theta)\n",
        "\n",
        "# -----------------------------\n",
        "# Bounds\n",
        "# -----------------------------\n",
        "bounds = [(1e-9,1e-6), (1e-8,1e-4), (1e-3,1), (1e-8,1e-4)]\n",
        "\n",
        "# -----------------------------\n",
        "# Estimate MLE\n",
        "# -----------------------------\n",
        "print(\"Running optimization...\")\n",
        "res_global = differential_evolution(neg_ll, bounds, maxiter=40, polish=False, workers=-1)\n",
        "res_local = minimize(neg_ll, res_global.x, bounds=bounds, method=\"L-BFGS-B\")\n",
        "theta_mle = res_local.x\n",
        "print(\"MLE:\", theta_mle)\n",
        "\n",
        "# -----------------------------\n",
        "# Profile likelihood\n",
        "# -----------------------------\n",
        "def profile_likelihood(param_index, theta_mle, bounds, n_grid=25):\n",
        "    p_grid = np.linspace(bounds[param_index][0], bounds[param_index][1], n_grid)\n",
        "    ll_values = np.empty_like(p_grid)\n",
        "    ll_mle = loglikelihood(theta_mle)\n",
        "    nuisance_guess = [theta_mle[i] for i in range(len(theta_mle)) if i != param_index]\n",
        "    for j, val in enumerate(tqdm(p_grid, desc=f\"Param {param_index+1}\")):\n",
        "        theta_fixed = theta_mle.copy()\n",
        "        theta_fixed[param_index] = val\n",
        "        free_idx = [i for i in range(len(theta_mle)) if i != param_index]\n",
        "        free_bounds = [bounds[i] for i in free_idx]\n",
        "        def neg_ll_nuisance(free_params):\n",
        "            theta_var = theta_fixed.copy()\n",
        "            theta_var[free_idx] = free_params\n",
        "            return -loglikelihood(theta_var)\n",
        "        res = minimize(neg_ll_nuisance, nuisance_guess, bounds=free_bounds,\n",
        "                       method=\"L-BFGS-B\", options={\"maxiter\":200})\n",
        "        ll_values[j] = -res.fun\n",
        "        nuisance_guess = res.x\n",
        "    norm_ll = ll_values - ll_mle\n",
        "    return param_index, p_grid, norm_ll\n",
        "\n",
        "# -----------------------------\n",
        "# Run all profiles\n",
        "# -----------------------------\n",
        "results = Parallel(n_jobs=4)(\n",
        "    delayed(profile_likelihood)(i, theta_mle, bounds, n_grid=25)\n",
        "    for i in range(4)\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Plot\n",
        "# -----------------------------\n",
        "cutoff = -chi2.ppf(0.95, df=1)/2\n",
        "labels = [r\"$\\phi_m$\", r\"$\\gamma_m$\", r\"$\\delta_m$\", r\"$\\eta_m$\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(7,8))\n",
        "axes = axes.ravel()\n",
        "cutoff = -chi2.ppf(0.95, df=1)/2\n",
        "\n",
        "for param_index, p_grid, norm_ll in results:\n",
        "    ax = axes[param_index]\n",
        "    ax.plot(p_grid, norm_ll, color='blue', lw=2, label=\"Profile\")\n",
        "    ax.plot(p_grid, norm_ll, 'ro', markeredgecolor='black', label=\"Steps\")\n",
        "    ax.axhline(cutoff, color='green', linestyle='--', label='95% CI cutoff')\n",
        "    ax.axvline(theta_mle[param_index], color='black', linestyle=':', label='MLE')\n",
        "    ax.set_xlabel(param_labels[param_index])\n",
        "    ax.set_ylabel(r\"$\\Delta$ log-likelihood\")\n",
        "    ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}